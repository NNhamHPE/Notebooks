{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e841e490",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20960ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61070607",
   "metadata": {},
   "source": [
    "Import the time module and create a variable named \"start\". The start variable will be used later to track the runtime of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdb3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "curTime = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Start Time: \", curTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053bcf3a",
   "metadata": {},
   "source": [
    "Use datetime to get the current time, used to display when the script has started running. Useful for calculating when an operation will end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sqla\n",
    "import io\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05d1b0",
   "metadata": {},
   "source": [
    "Import the remaining modules. The modules are used to the following:<br>\n",
    "-Pandas: Used for the dataframe datatype which is easy to operate on as well as convert to other files. It is also compatable with fitting for many ML models.<br>\n",
    "-Numpy: Used for various calculations and time conversions.<br>\n",
    "-SQLAlchemy: Used to import data from database and convert to a dataframe.<br>\n",
    "-io: Used to convert ANSI csv files to UTF-8 in order to make special characters readable.<br>\n",
    "-OneHotEncoder: Needed to convert categorical variables into dummy variables.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b0792",
   "metadata": {},
   "source": [
    "# Getting and converting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "runTimeBool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d847534",
   "metadata": {},
   "source": [
    "When set to true, messages will be printed to the console detailing the runtimes at each checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8987f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runTimeBool == True:\n",
    "    checkpoint = round(time.time()-start, 2)\n",
    "    print(f\"Module import runtime: {checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf443b1",
   "metadata": {},
   "source": [
    "Calculate the runtime of the script to that point, remember to round the float in the checkpoint operation in order to improve runtime preformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "getDataFromSQL = True\n",
    "if getDataFromSQL == True:\n",
    "    \n",
    "    server = \"hpeanneops.rose.rdlabs.hpecorp.net\"\n",
    "    username = \"ops_aruba_writer\"\n",
    "    password = \"W0nderfu1Pa55w0rd\"\n",
    "    db = \"NEO\"\n",
    "    driver = \"ODBC Driver 17 for SQL Server\"\n",
    "\n",
    "    dbconnection = f\"mssql://{username}:{password}@{server}/{db}?driver={driver}\"\n",
    "    engine = sqla.create_engine(dbconnection)\n",
    "    connection = engine.connect()\n",
    "\n",
    "    sqltopandasdf = pd.read_sql(\"SELECT\tR.ReqMasterId, R.EMPType, R.ReqStatus, R.NbofReqs, R.HiringManagerName, R.HiringManagerId, \\\n",
    "    R.REQApprovalDate, R.CostCenter, R.City, R.Country, R.JobFamily, R.JobCode, \\\n",
    "    (SELECT TOP  1 P.[month] + '-01' FROM HeadcountPlan P WHERE P.ReqNumber = R.ReqNumber ORDER BY P.[month] desc) AS StartDate \\\n",
    "    FROM\tRequisitionMaster R \\\n",
    "    WHERE\t(SELECT TOP  1 P.[month] + '-01' FROM HeadcountPlan P WHERE P.ReqNumber = R.ReqNumber ORDER BY P.[month] desc) IS NOT NULL \\\n",
    "    and R.ReqStatus <> 'Closed' and R.EMPType <> 'INT'\", engine)\n",
    "    \n",
    "    sqltopandasdf.to_csv(\"compiledata.csv\", index=False)\n",
    "    \n",
    "    if runTimeBool == True:\n",
    "        checkpoint = round(time.time()-start, 2)\n",
    "        print(f\"Database import runtime: {checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1bdfcd",
   "metadata": {},
   "source": [
    "When getDataFromSQL is set to True, the above code will use the input credential to access the NEO database and create an engine to read the database then convert it to a dataframe. The server, username, password, database, and SQL command can all be easily changed. The final result from the SQL pull will be exported to a csv. When getDataFromSQL is set to False, the script will try to pull data from an existing compiledata.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5879ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialcsv = \"compiledata.csv\"\n",
    "try:\n",
    "    with io.open(\"compiledata.csv\", encoding=\"ANSI\") as outer:\n",
    "        with io.open(\"convertedcompileddata.csv\", \"w\", encoding=\"UTF-8\") as inner:\n",
    "            for line in outer:\n",
    "                inner.write(line)\n",
    "except:\n",
    "    dummydf = pd.read_csv(\"convertedcompileddata.csv\", index_col=False)\n",
    "    dummydf.to_csv(\"convertedcompileddata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116477ea",
   "metadata": {},
   "source": [
    "Try to convert the data obtained from ANSI into UTF-8 encoding to avoid reading errors with pandas. End result is exported to csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d95d57",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDupeCols(pandasdf):\n",
    "    dupeCols = set()\n",
    "    for x in range(pandasdf.shape[1]):\n",
    "        col = pandasdf.iloc[:,x]\n",
    "        for y in range(x+1,pandasdf.shape[1]):\n",
    "            col2 = pandasdf.iloc[:,y]\n",
    "            if col.equals(col2):\n",
    "                dupeCols.add(pandasdf.columns.values[x])\n",
    "    return list(dupeCols)\n",
    "dupecols = findDupeCols(pandasdf)\n",
    "pandasdf.drop(columns = dupecols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42c0c5",
   "metadata": {},
   "source": [
    "Fucntion to loop through the values in each column and compare them to each other column. If the columns have identical values, then the columns will be dropped. This is to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb670ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in pandasdf.iterrows():\n",
    "    if row[\"ReqStatus\"].lower() == \"closed\":\n",
    "        pandasdf.drop(index, axis=0, inplace=True)\n",
    "pandasdf = pandasdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb4eae",
   "metadata": {},
   "source": [
    "Iterate over the dataframe and if the ReqStatus column is marked as \"closed\", drop the row. Currently made redundant by the initial SQL command. Mind the dataframe reset_index function, as it is required every time a row is dropped from a dataframe otherwise indexing errors will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clearCompleteDupeReqs = True\n",
    "alsoClearUncompleteDupes = False\n",
    "if clearCompleteDupeReqs == True:\n",
    "    dupeRecList = []\n",
    "    ucDupeRecList = []\n",
    "    for index, row in pandasdf.iterrows():\n",
    "        if row[\"NbofReqs\"] > 1:\n",
    "            concatRow = str(row[\"NbofReqs\"]) + str(row[\"City\"]) + str(row[\"HiringManagerName\"] + str(row[\"REQApprovalDate\"]))\n",
    "            if row[\"ReqStatus\"] == \"Filled\" or row[\"ReqStatus\"] == \"Filled in WD with open status\":\n",
    "                if concatRow in dupeRecList:\n",
    "                    pandasdf.drop(index, inplace=True)\n",
    "                else:\n",
    "                    dupeRecList.append(concatRow)\n",
    "            else:\n",
    "                if alsoClearUncompleteDupes == True:\n",
    "                    if concatRow in ucDupeRecList:\n",
    "                        pandasdf.drop(index, inplace=True)\n",
    "                    else:\n",
    "                        ucDupeRecList.append(concatRow)\n",
    "                else:\n",
    "                    pass\n",
    "pandasdf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668dea6",
   "metadata": {},
   "source": [
    "Check for reqs with the same NbofReqs, City, and HiringManager and drop them if they are the same in order to avoid overfitting and certain reqs affecting the model and target accuracy more than they should. Can change the two vaiables clearCompleteDupeReqs and alsoClearUncompleteDupes to change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc728ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if getDataFromSQL == True:\n",
    "    pandasdf[\"REQApprovalDate\"] = pd.to_datetime(pandasdf[\"REQApprovalDate\"], format=\"%Y-%m-%d\")\n",
    "    pandasdf[\"StartDate\"] = pd.to_datetime(pandasdf[\"StartDate\"], format=\"%Y-%m-%d\")\n",
    "else:\n",
    "    pandasdf[\"REQApprovalDate\"] = pd.to_datetime(pandasdf[\"REQApprovalDate\"], format=\"%m/%d/%Y\")\n",
    "    pandasdf[\"StartDate\"] = pd.to_datetime(pandasdf[\"StartDate\"], format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4d96f",
   "metadata": {},
   "source": [
    "Convert the dates pulled from SQL into pandas datetime which is operable by numpy. The conversion depends on the source, the dates pulled from SQL server will use the Y-m-d format while csv files use the m/d/Y format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4705307",
   "metadata": {},
   "outputs": [],
   "source": [
    "ununiqueCols = [x for x in pandasdf.columns if pandasdf[x].nunique()==1]\n",
    "pandasdf.drop(ununiqueCols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d52d94d3",
   "metadata": {},
   "source": [
    "Check for columns that only have one value across all their rows and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d57610",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgedate = \"1905-01-01 00:00:00\"\n",
    "edgedate64 = np.datetime64(edgedate)\n",
    "for date in pandasdf[\"StartDate\"]:\n",
    "    if date < edgedate64:\n",
    "        pandasdf.drop(pandasdf.index[(pandasdf[\"StartDate\"] == date)], axis=0, inplace=True)\n",
    "for date in pandasdf[\"REQApprovalDate\"]:\n",
    "    if date < edgedate64:\n",
    "        pandasdf.drop(pandasdf.index[(pandasdf[\"REQApprovalDate\"] == date)], axis=0, inplace=True)\n",
    "pandasdf = pandasdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4b829",
   "metadata": {},
   "source": [
    "Drop any rows with a StartDate or REQApprovalDate that is before Janurary 1st 1905. This date is arbitrary and is intended to cull any unreliable data using just the database default date (1901-01-01) for either StartDate or REQApprovalDate. This date could be changed to anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yTestStartDates = []\n",
    "for index, row in pandasdf.iterrows():\n",
    "    if row[\"ReqStatus\"].lower() == \"open\" or row[\"ReqStatus\"].lower() == \"in_progress\" or row[\"ReqStatus\"] == \"Frozen\":\n",
    "        yTestStartDates.append((row[\"StartDate\"]-row[\"REQApprovalDate\"]).days)\n",
    "        pandasdf.loc[index, \"StartDate\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6a34a",
   "metadata": {},
   "source": [
    "Get the age for the incomplete reqs based on Fred's predicted values, used for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b4d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nameList = []\n",
    "for index, row in pandasdf.iterrows():\n",
    "    if row[\"HiringManagerId\"] == \"On Leave\" and row[\"HiringManagerName\"] not in nameList:\n",
    "        nameList.append(row[\"HiringManagerName\"])\n",
    "idDict = {}\n",
    "for index, row in pandasdf.iterrows():\n",
    "    if row[\"HiringManagerName\"] in nameList and row[\"HiringManagerId\"].lower() != \"on leave\":\n",
    "        idDict[row[\"HiringManagerName\"]] = row[\"HiringManagerId\"]\n",
    "for index, row in pandasdf.iterrows():\n",
    "    if row[\"HiringManagerName\"] in idDict.keys() and row[\"HiringManagerId\"].lower() == \"on leave\":\n",
    "        row[\"HiringManagerId\"] = idDict.get(str(row[\"HiringManagerName\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff18b28",
   "metadata": {},
   "source": [
    "First, find reqs where the HiringManagerId is erroneously labeled as \"On Leave\" and store those in a list. Then, check if those names have another req ID associated with them. If they do, store the name and the ID in a dictionary with the name being the key and the ID being the value. Then iterate over the dataframe one last time and replace the IDs that say \"On Leave\" with the actual ID associated with the name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc40818",
   "metadata": {},
   "outputs": [],
   "source": [
    "createYearList = []\n",
    "createMonthList = []\n",
    "createDayList = []\n",
    "startYearList = []\n",
    "startMonthList = []\n",
    "startDayList = []\n",
    "for index, row in pandasdf.iterrows():\n",
    "    createYearList.append(row[\"REQApprovalDate\"].year)\n",
    "    createMonthList.append(row[\"REQApprovalDate\"].month)\n",
    "    #createDayList.append(row[\"REQApprovalDate\"].day)\n",
    "    startYearList.append(row[\"StartDate\"].year)\n",
    "    startMonthList.append(row[\"StartDate\"].month)\n",
    "    #startDayList.append(row[\"StartDate\"].day)\n",
    "pandasdf.insert(pandasdf.shape[1], \"creation year\", createYearList)\n",
    "pandasdf.insert(pandasdf.shape[1], \"creation month\", createMonthList)\n",
    "#pandasdf.insert(pandasdf.shape[1], \"creation day\", createDayList)\n",
    "pandasdf.insert(pandasdf.shape[1], \"start year\", startYearList)\n",
    "pandasdf.insert(pandasdf.shape[1], \"start month\", startMonthList)\n",
    "#pandasdf.insert(pandasdf.shape[1], \"start day\", startDayList)\n",
    "\n",
    "if runTimeBool == True:\n",
    "    checkpoint = round(time.time()-start, 2)\n",
    "    print(f\"Editing conversion Runtime: {checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e54db6",
   "metadata": {},
   "source": [
    "Convert the days, months, and years in StartDate and REQApprovalDate to their own columns. The operations converting creation and start day are commented out as they are currently not being used on the model as they will have a disproportionate amount of weight on the model and the results are sorted in 15 day buckets. Currently, none of these factors are being used in the model as they were found to be detrimental to the overall results due to causing overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73560670",
   "metadata": {},
   "outputs": [],
   "source": [
    "reqMasterIDList = []\n",
    "removeReqMasterId = True\n",
    "if removeReqMasterId == True:\n",
    "    reqMasterIDList  = pandasdf[\"ReqMasterId\"].tolist()\n",
    "    pandasdf.drop(\"ReqMasterId\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a23a78",
   "metadata": {},
   "source": [
    "Store the values in the ReqMasterId column in a list and drop the ReqMasterId column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oheBool = True\n",
    "if oheBool == True:\n",
    "    ohe = OneHotEncoder()\n",
    "    #pandasdf[\"start year\"] = (pandasdf[\"start year\"].astype(\"category\")).cat.codes\n",
    "    #pandasdf[\"start month\"] = (pandasdf[\"start month\"].astype(\"category\")).cat.codes\n",
    "    #pandasdf[\"ReqStatus\"] = (pandasdf[\"ReqStatus\"].astype(\"category\")).cat.codes\n",
    "    pandasdf[\"creation month\"] = (pandasdf[\"creation month\"].astype(\"category\")).cat.codes\n",
    "    pandasdf[\"HiringManagerId\"] = (pandasdf[\"HiringManagerId\"].astype(\"category\")).cat.codes\n",
    "    pandasdf[\"CostCenter\"] = (pandasdf[\"CostCenter\"].astype(\"category\")).cat.codes\n",
    "    pandasdf[\"City\"] = (pandasdf[\"City\"].astype(\"category\")).cat.codes\n",
    "    pandasdf[\"Country\"] = (pandasdf[\"Country\"].astype(\"category\")).cat.codes\n",
    "    #pandasdf[\"JobFamily\"] = (pandasdf[\"JobFamily\"].astype(\"category\")).cat.codes\n",
    "    pandasdf[\"JobCode\"] = (pandasdf[\"JobCode\"].astype(\"category\")).cat.codes\n",
    "\n",
    "    ohearray = pd.DataFrame(ohe.fit_transform(pandasdf[[#\"creation month\", #\"ReqStatus\",\n",
    "            \"HiringManagerId\", \"CostCenter\", \"City\",\n",
    "            \"Country\", \"JobCode\"]]).toarray())\n",
    "    pandasdf = pandasdf.join(ohearray)\n",
    "    pandasdf = pandasdf.drop([\"creation month\", #\"start month\", #\"ReqStatus\",\n",
    "            \"HiringManagerName\", \"NbofReqs\", \"HiringManagerId\", \"CostCenter\", \"City\",\n",
    "            \"Country\", \"JobCode\"], axis=1)\n",
    "\n",
    "if runTimeBool == True:\n",
    "    checkpoint = round(time.time()-start, 2)\n",
    "    print(f\"Encoding runtime: {checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16056391",
   "metadata": {},
   "source": [
    "OHE stands for One Hot Encoder, an encoding method that converts all categorical variables to their own binary columns. Often times required by most models to read categorical variables. First, convert the columns of the dataframe into categories, then convert the category columns into encoded dummy columns and attach them to the dataframe. Drop the original categorical columns. In this current iteration, the start year, start month, ReqStatus, and JobFamily conversions to category datatype are commented out and those columns are not encoded. For start month and start year, as previously mentioned, they are not included in the model because they hurt the model due to overfitting. ReqStatus is dropped because the ReqStatus of the predicted values cannot match the ReqStatus of the actual values. JobFamily is not used because it has been replaced by JobCode, which is more diverse and nuanced, as it includes the requirement of the position as well as the job type and tends to lead to more accurate results on a macro level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pandasdf[pandasdf.StartDate.isnull()].reset_index(drop=True)\n",
    "testData.insert(testData.shape[1], \"Age\", yTestStartDates)\n",
    "ageList = []\n",
    "for index, row in pandasdf.iterrows():\n",
    "    age = (row[\"StartDate\"] - row[\"REQApprovalDate\"]).days\n",
    "    if age >= 0:\n",
    "        ageList.append(age)\n",
    "    else:\n",
    "        pandasdf = pandasdf.drop(index=index)\n",
    "pandasdf.insert(pandasdf.shape[1], \"Age\", ageList)\n",
    "pandasdf = pandasdf.dropna(axis=0, subset=[\"StartDate\"])\n",
    "pandasdf = pandasdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d6c7c",
   "metadata": {},
   "source": [
    "First, copy the rows that do not have a StartDate to its own testData dataframe and drop those rows in the original dataframe. Then, attach the predicted values to the testData dataframe. Iterate over the original dataframe and subtract the StartDate from the REQApprovalDate to determine the age of the req. Reqs with a negative age (StartDate is incorrectly input before REQApprovalDate) are culled. This operation that calculates eats up the majority of the runtime for this script with more categorical variables and more rows. If possible, try to optimize to only consider the two columns it needs to calculate on rather than iterating over the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketList = []\n",
    "if pandasdf[\"Age\"].max() > testData[\"Age\"].max():\n",
    "    for i in range(0, pandasdf[\"Age\"].max()+15, 15):\n",
    "        bucketList.append(i)\n",
    "else:\n",
    "    for i in range(0, testData[\"Age\"].max()+15, 15):\n",
    "        bucketList.append(i)\n",
    "\n",
    "pandasdf[\"AgeGroups\"] = pd.cut(pandasdf[\"Age\"], bucketList, include_lowest=True)\n",
    "testData[\"AgeGroups\"] = pd.cut(testData[\"Age\"], bucketList, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e6859",
   "metadata": {},
   "source": [
    "After calculating age, create a list of 15 day buckets representing every possible outcome. Then, use that list in pandas' cut function which automatically sorts each value into their respective buckets. These categorical buckets are now the targets that the model will predict on going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ad31d",
   "metadata": {},
   "source": [
    "# Final Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d839c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = pandasdf.drop([\"Age\", \"StartDate\", \"start year\", \"REQApprovalDate\"], axis=1)\n",
    "yaxis = pandasdf[\"Age\"]\n",
    "xtest = testData.drop([\"Age\", \"StartDate\", \"start year\", \"REQApprovalDate\"], axis=1)\n",
    "ytest = testData[\"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b029c",
   "metadata": {},
   "source": [
    "Create new dataframes that are split between the variables and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "if oheBool == True:\n",
    "    xtest.columns = xtest.columns.astype(str)\n",
    "    xaxis.columns = xaxis.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c002f35",
   "metadata": {},
   "source": [
    "Convert the column names to string to avoid an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exportAxisAndTest = True\n",
    "if exportAxisAndTest == True:\n",
    "    xaxis.to_csv(\"xaxis.csv\", index=False, encoding=\"utf-8\")\n",
    "    xtest.to_csv(\"xtest.csv\", index=False, encoding=\"utf-8\")\n",
    "    yaxis.to_csv(\"yaxis.csv\", index=False, encoding=\"utf-8\")\n",
    "    ytest.to_csv(\"ytest.csv\", index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    if runTimeBool == True:\n",
    "        checkpoint = round(time.time()-start, 2)\n",
    "        print(f\"Export Test and Axis: {checkpoint}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e79623",
   "metadata": {},
   "source": [
    "Export dataframes to csv. These csvs will be called on and used in other scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878adcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
